#!/bin/bash
# OwLore è®­ç»ƒå’Œè¯„ä¼°è„šæœ¬
# ç®€å•ç›´æ¥ï¼ŒæŒ¨ä¸ªæ‰§è¡Œè®­ç»ƒå’Œè¯„ä¼°

echo "ğŸ¯ OwLore å®Œæ•´å®éªŒå¼€å§‹"
echo "ğŸ“… å¼€å§‹æ—¶é—´: $(date)"
echo ""

# æ¿€æ´»condaç¯å¢ƒçš„å‡½æ•°
activate_conda() {
    source /home/rtx3090/miniconda3/etc/profile.d/conda.sh
    conda activate $1
    echo "âœ… æ¿€æ´»ç¯å¢ƒ: $1"
}

# 1. LoRA æ¨¡å‹
echo "============================================================"
echo "ğŸ¯ å¼€å§‹è®­ç»ƒå’Œè¯„ä¼° LoRA æ¨¡å‹"
echo "============================================================"

echo "ğŸ‹ï¸ è®­ç»ƒ LoRA æ¨¡å‹..."
activate_conda owlore_fine
python examples/finetune.py \
    --model_name_or_path "meta-llama/Llama-3.2-1B" \
    --dataset_path "data/OwLore_Dataset/merge" \
    --output_dir "output_models/my_finetuned_model_lora" \
    --overwrite_output_dir \
    --num_train_epochs 1.0 \
    --learning_rate 3e-4 \
    --block_size 512 \
    --per_device_train_batch_size 8 \
    --gradient_accumulation_steps 1 \
    --bf16 \
    --torch_dtype bfloat16 \
    --run_name "LoRA_training" \
    --optim adamw_hf \
    --validation_split_percentage 0 \
    --logging_steps 1 \
    --do_train \
    --ddp_timeout 72000 \
    --save_steps 10000 \
    --dataloader_num_workers 1 \
    --gradient_checkpointing False \
    --use_flash_attention 1 \
    --use_lisa 0 \
    --disable_group_texts 0 \
    --seed 111 \
    --galore False \
    --lora_rank 8 \
    --lora_alpha_custom 16 \
    --cola_silu True \
    --cola_init False \
    --adapter_scope all \
    --sparsity 0.001 \
    --gamma 0.7 \
    --sparse_method svd \
    --sparse_svd_rank 256 \
    --svd_inverse False \
    --finetune_method lora

if [ $? -eq 0 ]; then
    echo "âœ… LoRA è®­ç»ƒå®Œæˆ"
    echo "ğŸ“Š è¯„ä¼° LoRA æ¨¡å‹..."
    activate_conda owlore_eval
    python run_owlore_eval.py \
        --model_path "output_models/my_finetuned_model_lora/complete_model" \
        --finetune_method lora \
        --adapter_scope all \
        --rank 8 \
        --alpha 16 \
        --sparsity 0.001 \
        --gamma 0.7 \
        --sparse_method svd \
        --sparse_svd_rank 256 \
        --cola_silu true \
        --cola_init false \
        --svd_inverse false \
        --tasks "boolq,piqa,hellaswag,winogrande,arc_easy,arc_challenge,openbookqa" \
        --output_path "final_results/LoRA_results" \
        --num_fewshot 5 \
        --batch_size auto
    
    if [ $? -eq 0 ]; then
        echo "âœ… LoRA è¯„ä¼°å®Œæˆ"
    else
        echo "âŒ LoRA è¯„ä¼°å¤±è´¥"
    fi
else
    echo "âŒ LoRA è®­ç»ƒå¤±è´¥ï¼Œè·³è¿‡è¯„ä¼°"
fi

echo ""
echo "ğŸ˜´ ä¼‘æ¯30ç§’..."
sleep 30

# 2. ELSA_SiLU_True æ¨¡å‹
echo "============================================================"
echo "ğŸ¯ å¼€å§‹è®­ç»ƒå’Œè¯„ä¼° ELSA_SiLU_True æ¨¡å‹"
echo "============================================================"

echo "ğŸ‹ï¸ è®­ç»ƒ ELSA_SiLU_True æ¨¡å‹..."
activate_conda owlore_fine
python examples/finetune.py \
    --model_name_or_path "meta-llama/Llama-3.2-1B" \
    --dataset_path "data/OwLore_Dataset/merge" \
    --output_dir "output_models/my_finetuned_model_elsa_silutrue" \
    --overwrite_output_dir \
    --num_train_epochs 1.0 \
    --learning_rate 3e-4 \
    --block_size 512 \
    --per_device_train_batch_size 8 \
    --gradient_accumulation_steps 1 \
    --bf16 \
    --torch_dtype bfloat16 \
    --run_name "ELSA_SiLU_True_training" \
    --optim adamw_hf \
    --validation_split_percentage 0 \
    --logging_steps 1 \
    --do_train \
    --ddp_timeout 72000 \
    --save_steps 10000 \
    --dataloader_num_workers 1 \
    --gradient_checkpointing False \
    --use_flash_attention 1 \
    --use_lisa 0 \
    --disable_group_texts 0 \
    --seed 111 \
    --galore False \
    --lora_rank 7 \
    --lora_alpha_custom 14 \
    --cola_silu True \
    --cola_init False \
    --adapter_scope all \
    --sparsity 0.001 \
    --gamma 0.7 \
    --sparse_method svd \
    --sparse_svd_rank 256 \
    --svd_inverse False \
    --finetune_method elsa

if [ $? -eq 0 ]; then
    echo "âœ… ELSA_SiLU_True è®­ç»ƒå®Œæˆ"
    echo "ğŸ“Š è¯„ä¼° ELSA_SiLU_True æ¨¡å‹..."
    activate_conda owlore_eval
    python run_owlore_eval.py \
        --model_path "output_models/my_finetuned_model_elsa_silutrue/complete_model" \
        --finetune_method elsa \
        --adapter_scope all \
        --rank 7 \
        --alpha 14 \
        --sparsity 0.001 \
        --gamma 0.7 \
        --sparse_method svd \
        --sparse_svd_rank 256 \
        --cola_silu true \
        --cola_init false \
        --svd_inverse false \
        --tasks "boolq,piqa,hellaswag,winogrande,arc_easy,arc_challenge,openbookqa" \
        --output_path "final_results/ELSA_SiLU_True_results" \
        --num_fewshot 5 \
        --batch_size auto
    
    if [ $? -eq 0 ]; then
        echo "âœ… ELSA_SiLU_True è¯„ä¼°å®Œæˆ"
    else
        echo "âŒ ELSA_SiLU_True è¯„ä¼°å¤±è´¥"
    fi
else
    echo "âŒ ELSA_SiLU_True è®­ç»ƒå¤±è´¥ï¼Œè·³è¿‡è¯„ä¼°"
fi

echo ""
echo "ğŸ˜´ ä¼‘æ¯30ç§’..."
sleep 30

# 3. ELSA_SiLU_False æ¨¡å‹
echo "============================================================"
echo "ğŸ¯ å¼€å§‹è®­ç»ƒå’Œè¯„ä¼° ELSA_SiLU_False æ¨¡å‹"
echo "============================================================"

echo "ğŸ‹ï¸ è®­ç»ƒ ELSA_SiLU_False æ¨¡å‹..."
activate_conda owlore_fine
python examples/finetune.py \
    --model_name_or_path "meta-llama/Llama-3.2-1B" \
    --dataset_path "data/OwLore_Dataset/merge" \
    --output_dir "output_models/my_finetuned_model_elsa_silufalse" \
    --overwrite_output_dir \
    --num_train_epochs 1.0 \
    --learning_rate 3e-4 \
    --block_size 512 \
    --per_device_train_batch_size 8 \
    --gradient_accumulation_steps 1 \
    --bf16 \
    --torch_dtype bfloat16 \
    --run_name "ELSA_SiLU_False_training" \
    --optim adamw_hf \
    --validation_split_percentage 0 \
    --logging_steps 1 \
    --do_train \
    --ddp_timeout 72000 \
    --save_steps 10000 \
    --dataloader_num_workers 1 \
    --gradient_checkpointing False \
    --use_flash_attention 1 \
    --use_lisa 0 \
    --disable_group_texts 0 \
    --seed 111 \
    --galore False \
    --lora_rank 7 \
    --lora_alpha_custom 14 \
    --cola_silu False \
    --cola_init False \
    --adapter_scope all \
    --sparsity 0.001 \
    --gamma 0.7 \
    --sparse_method svd \
    --sparse_svd_rank 256 \
    --svd_inverse False \
    --finetune_method elsa

if [ $? -eq 0 ]; then
    echo "âœ… ELSA_SiLU_False è®­ç»ƒå®Œæˆ"
    echo "ğŸ“Š è¯„ä¼° ELSA_SiLU_False æ¨¡å‹..."
    activate_conda owlore_eval
    python run_owlore_eval.py \
        --model_path "output_models/my_finetuned_model_elsa_silufalse/complete_model" \
        --finetune_method elsa \
        --adapter_scope all \
        --rank 7 \
        --alpha 14 \
        --sparsity 0.001 \
        --gamma 0.7 \
        --sparse_method svd \
        --sparse_svd_rank 256 \
        --cola_silu false \
        --cola_init false \
        --svd_inverse false \
        --tasks "boolq,piqa,hellaswag,winogrande,arc_easy,arc_challenge,openbookqa" \
        --output_path "final_results/ELSA_SiLU_False_results" \
        --num_fewshot 5 \
        --batch_size auto
    
    if [ $? -eq 0 ]; then
        echo "âœ… ELSA_SiLU_False è¯„ä¼°å®Œæˆ"
    else
        echo "âŒ ELSA_SiLU_False è¯„ä¼°å¤±è´¥"
    fi
else
    echo "âŒ ELSA_SiLU_False è®­ç»ƒå¤±è´¥ï¼Œè·³è¿‡è¯„ä¼°"
fi

echo ""
echo "ğŸ˜´ ä¼‘æ¯30ç§’..."
sleep 30

# 4. COLA æ¨¡å‹
echo "============================================================"
echo "ğŸ¯ å¼€å§‹è®­ç»ƒå’Œè¯„ä¼° COLA æ¨¡å‹"
echo "============================================================"

echo "ğŸ‹ï¸ è®­ç»ƒ COLA æ¨¡å‹..."
activate_conda owlore_fine
python examples/finetune.py \
    --model_name_or_path "meta-llama/Llama-3.2-1B" \
    --dataset_path "data/OwLore_Dataset/merge" \
    --output_dir "output_models/my_finetuned_model_cola" \
    --overwrite_output_dir \
    --num_train_epochs 1.0 \
    --learning_rate 3e-4 \
    --block_size 512 \
    --per_device_train_batch_size 8 \
    --gradient_accumulation_steps 1 \
    --bf16 \
    --torch_dtype bfloat16 \
    --run_name "COLA_training" \
    --optim adamw_hf \
    --validation_split_percentage 0 \
    --logging_steps 1 \
    --do_train \
    --ddp_timeout 72000 \
    --save_steps 10000 \
    --dataloader_num_workers 1 \
    --gradient_checkpointing False \
    --use_flash_attention 1 \
    --use_lisa 0 \
    --disable_group_texts 0 \
    --seed 111 \
    --galore False \
    --lora_rank 8 \
    --lora_alpha_custom 16 \
    --cola_silu True \
    --cola_init True \
    --adapter_scope all \
    --sparsity 0.001 \
    --gamma 0.7 \
    --sparse_method svd \
    --sparse_svd_rank 256 \
    --svd_inverse False \
    --finetune_method cola

if [ $? -eq 0 ]; then
    echo "âœ… COLA è®­ç»ƒå®Œæˆ"
    echo "ğŸ“Š è¯„ä¼° COLA æ¨¡å‹..."
    activate_conda owlore_eval
    python run_owlore_eval.py \
        --model_path "output_models/my_finetuned_model_cola/complete_model" \
        --finetune_method cola \
        --adapter_scope all \
        --rank 8 \
        --alpha 16 \
        --sparsity 0.001 \
        --gamma 0.7 \
        --sparse_method svd \
        --sparse_svd_rank 256 \
        --cola_silu true \
        --cola_init true \
        --svd_inverse false \
        --tasks "boolq,piqa,hellaswag,winogrande,arc_easy,arc_challenge,openbookqa" \
        --output_path "final_results/COLA_results" \
        --num_fewshot 5 \
        --batch_size auto
    
    if [ $? -eq 0 ]; then
        echo "âœ… COLA è¯„ä¼°å®Œæˆ"
    else
        echo "âŒ COLA è¯„ä¼°å¤±è´¥"
    fi
else
    echo "âŒ COLA è®­ç»ƒå¤±è´¥ï¼Œè·³è¿‡è¯„ä¼°"
fi

echo ""
echo "ğŸ˜´ ä¼‘æ¯30ç§’..."
sleep 30

# 5. LORO æ¨¡å‹
echo "============================================================"
echo "ğŸ¯ å¼€å§‹è®­ç»ƒå’Œè¯„ä¼° LORO æ¨¡å‹"
echo "============================================================"

echo "ğŸ‹ï¸ è®­ç»ƒ LORO æ¨¡å‹..."
activate_conda owlore_fine
python examples/finetune.py \
    --model_name_or_path "meta-llama/Llama-3.2-1B" \
    --dataset_path "data/OwLore_Dataset/merge" \
    --output_dir "output_models/my_finetuned_model_loro" \
    --overwrite_output_dir \
    --num_train_epochs 1.0 \
    --learning_rate 3e-4 \
    --block_size 512 \
    --per_device_train_batch_size 8 \
    --gradient_accumulation_steps 1 \
    --bf16 \
    --torch_dtype bfloat16 \
    --run_name "LORO_training" \
    --optim adamw_hf \
    --validation_split_percentage 0 \
    --logging_steps 1 \
    --do_train \
    --ddp_timeout 72000 \
    --save_steps 10000 \
    --dataloader_num_workers 1 \
    --gradient_checkpointing False \
    --use_flash_attention 1 \
    --use_lisa 0 \
    --disable_group_texts 0 \
    --seed 111 \
    --galore False \
    --lora_rank 8 \
    --lora_alpha_custom 16 \
    --cola_silu True \
    --cola_init False \
    --adapter_scope all \
    --sparsity 0.001 \
    --gamma 0.7 \
    --sparse_method svd \
    --sparse_svd_rank 256 \
    --svd_inverse False \
    --finetune_method loro

if [ $? -eq 0 ]; then
    echo "âœ… LORO è®­ç»ƒå®Œæˆ"
    echo "ğŸ“Š è¯„ä¼° LORO æ¨¡å‹..."
    activate_conda owlore_eval
    python run_owlore_eval.py \
        --model_path "output_models/my_finetuned_model_loro/complete_model" \
        --finetune_method loro \
        --adapter_scope all \
        --rank 8 \
        --alpha 16 \
        --sparsity 0.001 \
        --gamma 0.7 \
        --sparse_method svd \
        --sparse_svd_rank 256 \
        --cola_silu true \
        --cola_init false \
        --svd_inverse false \
        --tasks "boolq,piqa,hellaswag,winogrande,arc_easy,arc_challenge,openbookqa" \
        --output_path "final_results/LORO_results" \
        --num_fewshot 5 \
        --batch_size auto
    
    if [ $? -eq 0 ]; then
        echo "âœ… LORO è¯„ä¼°å®Œæˆ"
    else
        echo "âŒ LORO è¯„ä¼°å¤±è´¥"
    fi
else
    echo "âŒ LORO è®­ç»ƒå¤±è´¥ï¼Œè·³è¿‡è¯„ä¼°"
fi

echo ""
echo "ğŸ‰ æ‰€æœ‰å®éªŒå®Œæˆ!"
echo "ğŸ“… å®Œæˆæ—¶é—´: $(date)"
echo "ğŸ“ æŸ¥çœ‹ç»“æœ:"
echo "  - è®­ç»ƒæ¨¡å‹: output_models/"
echo "  - è¯„ä¼°ç»“æœ: final_results/"