#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
OwLore è‡ªåŠ¨åŒ–è®­ç»ƒå’Œè¯„ä¼°è„šæœ¬
æŒ‰é¡ºåºè®­ç»ƒ5ä¸ªæ¨¡å‹ï¼Œç„¶åæŒ‰é¡ºåºè¯„ä¼°æ¯ä¸ªæ¨¡å‹
"""

import os
import subprocess
import time
import json
from datetime import datetime

# è®­ç»ƒé…ç½®
TRAINING_CONFIGS = [
    {
        "name": "LoRA",
        "method": "lora", 
        "output_dir": "output_models/my_finetuned_model_lora",
        "rank": 8,
        "alpha": 16,
        "cola_silu": True,
        "cola_init": False,
        "dataset": "merge",  # mergeæ•°æ®é›†
        "test_tasks": "boolq,piqa,hellaswag,winogrande,arc_easy,arc_challenge,openbookqa"
    },
    {
        "name": "ELSA_SiLU_True",
        "method": "elsa",
        "output_dir": "output_models/my_finetuned_model_elsa_silutrue", 
        "rank": 7,
        "alpha": 14,
        "cola_silu": True,
        "cola_init": False,
        "dataset": "merge",  # mergeæ•°æ®é›†
        "test_tasks": "boolq,piqa,hellaswag,winogrande,arc_easy,arc_challenge,openbookqa"
    },
    {
        "name": "ELSA_SiLU_False",
        "method": "elsa",
        "output_dir": "output_models/my_finetuned_model_elsa_silufalse",
        "rank": 7, 
        "alpha": 14,
        "cola_silu": False,
        "cola_init": False,
        "dataset": "merge",  # mergeæ•°æ®é›†
        "test_tasks": "boolq,piqa,hellaswag,winogrande,arc_easy,arc_challenge,openbookqa"
    },
    {
        "name": "COLA",
        "method": "cola",
        "output_dir": "output_models/my_finetuned_model_cola",
        "rank": 8,
        "alpha": 16,
        "cola_silu": True,
        "cola_init": True,
        "dataset": "merge",  # mergeæ•°æ®é›†  
        "test_tasks": "boolq,piqa,hellaswag,winogrande,arc_easy,arc_challenge,openbookqa"
    },
    {
        "name": "LORO",
        "method": "loro",
        "output_dir": "output_models/my_finetuned_model_loro",
        "rank": 8,
        "alpha": 16, 
        "cola_silu": True,
        "cola_init": False,
        "dataset": "merge",  # mergeæ•°æ®é›†
        "test_tasks": "boolq,piqa,hellaswag,winogrande,arc_easy,arc_challenge,openbookqa"
    }
]

# åŸºç¡€è®­ç»ƒå‚æ•°
BASE_TRAIN_ARGS = [
    "--model_name_or_path", "meta-llama/Llama-3.2-1B",
    "--dataset_path", "data/OwLore_Dataset/merge",
    "--overwrite_output_dir",
    "--num_train_epochs", "1.0",
    "--learning_rate", "3e-4",
    "--block_size", "512", 
    "--per_device_train_batch_size", "8",
    "--gradient_accumulation_steps", "1",
    "--bf16",
    "--torch_dtype", "bfloat16",
    "--run_name", "my_finetuning_run",
    "--optim", "adamw_hf",
    "--validation_split_percentage", "0",
    "--logging_steps", "1",
    "--do_train",
    "--ddp_timeout", "72000",
    "--save_steps", "10000",
    "--dataloader_num_workers", "1",
    "--gradient_checkpointing", "False",
    "--use_flash_attention", "1",
    "--use_lisa", "0", 
    "--disable_group_texts", "0",
    "--seed", "111",
    "--galore", "False",
    "--adapter_scope", "all",
    "--sparsity", "0.001",
    "--gamma", "0.7",
    "--sparse_method", "svd",
    "--sparse_svd_rank", "256",
    "--svd_inverse", "False"
]

def run_command(cmd, description, log_file=None):
    """è¿è¡Œå‘½ä»¤å¹¶è®°å½•è¾“å‡º"""
    print(f"\n{'='*60}")
    print(f"å¼€å§‹æ‰§è¡Œ: {description}")
    print(f"å‘½ä»¤: {' '.join(cmd)}")
    print(f"{'='*60}")
    
    start_time = time.time()
    
    try:
        if log_file:
            with open(log_file, 'w') as f:
                process = subprocess.Popen(
                    cmd, 
                    stdout=subprocess.PIPE, 
                    stderr=subprocess.STDOUT,
                    universal_newlines=True,
                    bufsize=1
                )
                
                # å®æ—¶è¾“å‡ºå¹¶å†™å…¥æ—¥å¿—
                for line in process.stdout:
                    print(line.strip())
                    f.write(line)
                    f.flush()
                
                process.wait()
                return_code = process.returncode
        else:
            result = subprocess.run(cmd, capture_output=True, text=True)
            return_code = result.returncode
            if result.stdout:
                print(result.stdout)
            if result.stderr:
                print(result.stderr)
        
        end_time = time.time()
        duration = end_time - start_time
        
        if return_code == 0:
            print(f"âœ… {description} å®Œæˆ! è€—æ—¶: {duration:.2f}ç§’")
            return True
        else:
            print(f"âŒ {description} å¤±è´¥! è¿”å›ç : {return_code}")
            return False
            
    except Exception as e:
        print(f"âŒ æ‰§è¡Œ {description} æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        return False

def train_model(config):
    """è®­ç»ƒå•ä¸ªæ¨¡å‹"""
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ {config['name']} æ¨¡å‹...")
    
    # æ„å»ºè®­ç»ƒå‘½ä»¤
    train_cmd = ["python", "examples/finetune.py"] + BASE_TRAIN_ARGS + [
        "--output_dir", config["output_dir"],
        "--lora_rank", str(config["rank"]),
        "--lora_alpha_custom", str(config["alpha"]),
        "--cola_silu", str(config["cola_silu"]),
        "--cola_init", str(config["cola_init"]),
        "--finetune_method", config["method"]
    ]
    
    # åˆ›å»ºæ—¥å¿—æ–‡ä»¶
    log_dir = "experiment_logs"
    os.makedirs(log_dir, exist_ok=True)
    log_file = f"{log_dir}/train_{config['name']}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    
    success = run_command(
        train_cmd, 
        f"è®­ç»ƒ {config['name']} æ¨¡å‹",
        log_file
    )
    
    if success:
        # æ£€æŸ¥complete_modelæ˜¯å¦å­˜åœ¨
        complete_model_path = os.path.join(config["output_dir"], "complete_model")
        if os.path.exists(complete_model_path):
            print(f"âœ… {config['name']} æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œcomplete_modelå·²ä¿å­˜")
            return True
        else:
            print(f"âš ï¸ {config['name']} æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œä½†æ‰¾ä¸åˆ°complete_modelç›®å½•")
            return False
    
    return False

def evaluate_model(config):
    """è¯„ä¼°å•ä¸ªæ¨¡å‹"""
    print(f"\nğŸ“Š å¼€å§‹è¯„ä¼° {config['name']} æ¨¡å‹...")
    
    # æŸ¥æ‰¾æœ€æ–°çš„checkpointä¸­çš„complete_model
    output_dir = config["output_dir"]
    complete_model_path = None
    
    # å…ˆæ£€æŸ¥ç›´æ¥çš„complete_modelç›®å½•
    direct_complete_model = os.path.join(output_dir, "complete_model")
    if os.path.exists(direct_complete_model):
        complete_model_path = direct_complete_model
    else:
        # æŸ¥æ‰¾checkpointç›®å½•ä¸­çš„complete_model
        if os.path.exists(output_dir):
            checkpoints = [d for d in os.listdir(output_dir) if d.startswith("checkpoint-")]
            if checkpoints:
                # æŒ‰checkpointç¼–å·æ’åºï¼Œå–æœ€æ–°çš„
                checkpoints.sort(key=lambda x: int(x.split("-")[1]))
                latest_checkpoint = checkpoints[-1]
                checkpoint_complete_model = os.path.join(output_dir, latest_checkpoint, "complete_model")
                if os.path.exists(checkpoint_complete_model):
                    complete_model_path = checkpoint_complete_model
    
    if not complete_model_path:
        print(f"âŒ æ‰¾ä¸åˆ° {config['name']} æ¨¡å‹çš„complete_modelç›®å½•")
        return False
    
    print(f"ğŸ“‚ ä½¿ç”¨æ¨¡å‹è·¯å¾„: {complete_model_path}")
    
    # æ„å»ºè¯„ä¼°å‘½ä»¤
    eval_cmd = [
        "python", "run_owlore_eval.py",
        "--model_path", complete_model_path,
        "--finetune_method", config["method"],
        "--adapter_scope", "all",
        "--rank", str(config["rank"]),
        "--alpha", str(config["alpha"]),
        "--sparsity", "0.001",
        "--gamma", "0.7",
        "--sparse_method", "svd",
        "--sparse_svd_rank", "256",
        "--cola_silu", str(config["cola_silu"]).lower(),
        "--cola_init", str(config["cola_init"]).lower(),
        "--svd_inverse", "false",
        "--tasks", config["test_tasks"],
        "--output_path", f"experiment_results/{config['name']}_results",
        "--num_fewshot", "5",
        "--batch_size", "auto"
    ]
    
    # åˆ›å»ºç»“æœç›®å½•
    os.makedirs("experiment_results", exist_ok=True)
    
    # åˆ›å»ºæ—¥å¿—æ–‡ä»¶
    log_dir = "experiment_logs"
    log_file = f"{log_dir}/eval_{config['name']}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    
    success = run_command(
        eval_cmd,
        f"è¯„ä¼° {config['name']} æ¨¡å‹",
        log_file
    )
    
    return success

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ OwLore è‡ªåŠ¨åŒ–è®­ç»ƒå’Œè¯„ä¼°å¼€å§‹!")
    print(f"ğŸ“… å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # åˆ›å»ºå¿…è¦çš„ç›®å½•
    os.makedirs("experiment_logs", exist_ok=True)
    os.makedirs("experiment_results", exist_ok=True)
    
    # è®°å½•å®éªŒé…ç½®
    with open("experiment_logs/experiment_config.json", "w") as f:
        json.dump({
            "start_time": datetime.now().isoformat(),
            "configs": TRAINING_CONFIGS
        }, f, indent=2, ensure_ascii=False)
    
    # é˜¶æ®µ1: è®­ç»ƒæ‰€æœ‰æ¨¡å‹
    print("\n" + "="*80)
    print("ğŸ‹ï¸ é˜¶æ®µ1: å¼€å§‹è®­ç»ƒæ‰€æœ‰æ¨¡å‹")
    print("="*80)
    
    training_results = {}
    for i, config in enumerate(TRAINING_CONFIGS, 1):
        print(f"\nğŸ“‹ è®­ç»ƒè¿›åº¦: {i}/{len(TRAINING_CONFIGS)}")
        success = train_model(config)
        training_results[config['name']] = success
        
        if not success:
            print(f"âŒ {config['name']} è®­ç»ƒå¤±è´¥ï¼Œç»§ç»­ä¸‹ä¸€ä¸ªæ¨¡å‹...")
        
        # è®­ç»ƒé—´éš”ï¼Œè®©GPUä¼‘æ¯ä¸€ä¸‹
        if i < len(TRAINING_CONFIGS):
            print("ğŸ˜´ ç­‰å¾…30ç§’è®©GPUä¼‘æ¯...")
            time.sleep(30)
    
    # é˜¶æ®µ2: è¯„ä¼°æ‰€æœ‰æ¨¡å‹
    print("\n" + "="*80)
    print("ğŸ“Š é˜¶æ®µ2: å¼€å§‹è¯„ä¼°æ‰€æœ‰æ¨¡å‹")
    print("="*80)
    
    evaluation_results = {}
    for i, config in enumerate(TRAINING_CONFIGS, 1):
        print(f"\nğŸ“‹ è¯„ä¼°è¿›åº¦: {i}/{len(TRAINING_CONFIGS)}")
        
        if not training_results[config['name']]:
            print(f"â­ï¸ è·³è¿‡ {config['name']} è¯„ä¼° (è®­ç»ƒå¤±è´¥)")
            evaluation_results[config['name']] = False
            continue
        
        success = evaluate_model(config)
        evaluation_results[config['name']] = success
        
        # è¯„ä¼°é—´éš”
        if i < len(TRAINING_CONFIGS):
            print("ğŸ˜´ ç­‰å¾…10ç§’...")
            time.sleep(10)
    
    # æœ€ç»ˆæŠ¥å‘Š
    print("\n" + "="*80)
    print("ğŸ“‹ å®éªŒå®ŒæˆæŠ¥å‘Š")
    print("="*80)
    
    print("\nğŸ‹ï¸ è®­ç»ƒç»“æœ:")
    for name, success in training_results.items():
        status = "âœ… æˆåŠŸ" if success else "âŒ å¤±è´¥"
        print(f"  {name}: {status}")
    
    print("\nğŸ“Š è¯„ä¼°ç»“æœ:")
    for name, success in evaluation_results.items():
        if name not in training_results or not training_results[name]:
            status = "â­ï¸ è·³è¿‡"
        else:
            status = "âœ… æˆåŠŸ" if success else "âŒ å¤±è´¥"
        print(f"  {name}: {status}")
    
    # ä¿å­˜æœ€ç»ˆç»“æœ
    final_results = {
        "end_time": datetime.now().isoformat(),
        "training_results": training_results,
        "evaluation_results": evaluation_results
    }
    
    with open("experiment_logs/final_results.json", "w") as f:
        json.dump(final_results, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ‰ æ‰€æœ‰å®éªŒå®Œæˆ! ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("ğŸ“ æ—¥å¿—æ–‡ä»¶ä¿å­˜åœ¨: experiment_logs/")
    print("ğŸ“ ç»“æœæ–‡ä»¶ä¿å­˜åœ¨: experiment_results/")

if __name__ == "__main__":
    main()