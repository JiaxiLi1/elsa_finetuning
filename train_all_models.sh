#!/bin/bash
# OwLore цЙ╣щЗПшонч╗ГшДЪцЬм

echo "ЁЯЪА х╝АхзЛцЙ╣щЗПшонч╗ГOwLoreцибхЮЛ..."
echo "х╝АхзЛцЧ╢щЧ┤: $(date)"

# хИЫх╗║цЧех┐ЧчЫох╜Х
mkdir -p experiment_logs

# 1. шонч╗ГLoRAцибхЮЛ
echo ""
echo "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "="
echo "ЁЯПЛя╕П шонч╗ГLoRAцибхЮЛ (1/5)"
echo "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "="
python examples/finetune.py \
    --model_name_or_path "meta-llama/Llama-3.2-1B" \
    --dataset_path "data/OwLore_Dataset/merge" \
    --output_dir output_models/my_finetuned_model_lora \
    --overwrite_output_dir \
    --num_train_epochs 1.0 \
    --learning_rate 3e-4 \
    --block_size 512 \
    --per_device_train_batch_size 8 \
    --gradient_accumulation_steps 1 \
    --bf16 \
    --torch_dtype bfloat16 \
    --run_name my_finetuning_run \
    --optim adamw_hf \
    --validation_split_percentage 0 \
    --logging_steps 1 \
    --do_train \
    --ddp_timeout 72000 \
    --save_steps 10000 \
    --dataloader_num_workers 1 \
    --gradient_checkpointing False \
    --use_flash_attention 1 \
    --use_lisa 0 \
    --disable_group_texts 0 \
    --seed 111 \
    --galore False \
    --lora_rank 8 \
    --lora_alpha_custom 16 \
    --cola_silu True \
    --cola_init False \
    --adapter_scope all \
    --sparsity 0.001 \
    --gamma 0.7 \
    --sparse_method svd \
    --sparse_svd_rank 256 \
    --svd_inverse False \
    --finetune_method lora \
    2>&1 | tee experiment_logs/train_lora_$(date +%Y%m%d_%H%M%S).log

echo "тЬЕ LoRAцибхЮЛшонч╗ГхоМцИР"
sleep 30

# 2. шонч╗ГELSAцибхЮЛ (SiLU=True)
echo ""
echo "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "="
echo "ЁЯПЛя╕П шонч╗ГELSAцибхЮЛ SiLU=True (2/5)"
echo "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "="
python examples/finetune.py \
    --model_name_or_path "meta-llama/Llama-3.2-1B" \
    --dataset_path "data/OwLore_Dataset/merge" \
    --output_dir output_models/my_finetuned_model_elsa_silutrue \
    --overwrite_output_dir \
    --num_train_epochs 1.0 \
    --learning_rate 3e-4 \
    --block_size 512 \
    --per_device_train_batch_size 8 \
    --gradient_accumulation_steps 1 \
    --bf16 \
    --torch_dtype bfloat16 \
    --run_name my_finetuning_run \
    --optim adamw_hf \
    --validation_split_percentage 0 \
    --logging_steps 1 \
    --do_train \
    --ddp_timeout 72000 \
    --save_steps 10000 \
    --dataloader_num_workers 1 \
    --gradient_checkpointing False \
    --use_flash_attention 1 \
    --use_lisa 0 \
    --disable_group_texts 0 \
    --seed 111 \
    --galore False \
    --lora_rank 7 \
    --lora_alpha_custom 14 \
    --cola_silu True \
    --cola_init False \
    --adapter_scope all \
    --sparsity 0.001 \
    --gamma 0.7 \
    --sparse_method svd \
    --sparse_svd_rank 256 \
    --svd_inverse False \
    --finetune_method elsa \
    2>&1 | tee experiment_logs/train_elsa_silutrue_$(date +%Y%m%d_%H%M%S).log

echo "тЬЕ ELSA SiLU=TrueцибхЮЛшонч╗ГхоМцИР"
sleep 30

# 3. шонч╗ГELSAцибхЮЛ (SiLU=False)
echo ""
echo "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "="
echo "ЁЯПЛя╕П шонч╗ГELSAцибхЮЛ SiLU=False (3/5)"
echo "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "="
python examples/finetune.py \
    --model_name_or_path "meta-llama/Llama-3.2-1B" \
    --dataset_path "data/OwLore_Dataset/merge" \
    --output_dir output_models/my_finetuned_model_elsa_silufalse \
    --overwrite_output_dir \
    --num_train_epochs 1.0 \
    --learning_rate 3e-4 \
    --block_size 512 \
    --per_device_train_batch_size 8 \
    --gradient_accumulation_steps 1 \
    --bf16 \
    --torch_dtype bfloat16 \
    --run_name my_finetuning_run \
    --optim adamw_hf \
    --validation_split_percentage 0 \
    --logging_steps 1 \
    --do_train \
    --ddp_timeout 72000 \
    --save_steps 10000 \
    --dataloader_num_workers 1 \
    --gradient_checkpointing False \
    --use_flash_attention 1 \
    --use_lisa 0 \
    --disable_group_texts 0 \
    --seed 111 \
    --galore False \
    --lora_rank 7 \
    --lora_alpha_custom 14 \
    --cola_silu False \
    --cola_init False \
    --adapter_scope all \
    --sparsity 0.001 \
    --gamma 0.7 \
    --sparse_method svd \
    --sparse_svd_rank 256 \
    --svd_inverse False \
    --finetune_method elsa \
    2>&1 | tee experiment_logs/train_elsa_silufalse_$(date +%Y%m%d_%H%M%S).log

echo "тЬЕ ELSA SiLU=FalseцибхЮЛшонч╗ГхоМцИР"
sleep 30

# 4. шонч╗ГCOLAцибхЮЛ
echo ""
echo "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "="
echo "ЁЯПЛя╕П шонч╗ГCOLAцибхЮЛ (4/5)"
echo "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "="
python examples/finetune.py \
    --model_name_or_path "meta-llama/Llama-3.2-1B" \
    --dataset_path "data/OwLore_Dataset/merge" \
    --output_dir output_models/my_finetuned_model_cola \
    --overwrite_output_dir \
    --num_train_epochs 1.0 \
    --learning_rate 3e-4 \
    --block_size 512 \
    --per_device_train_batch_size 8 \
    --gradient_accumulation_steps 1 \
    --bf16 \
    --torch_dtype bfloat16 \
    --run_name my_finetuning_run \
    --optim adamw_hf \
    --validation_split_percentage 0 \
    --logging_steps 1 \
    --do_train \
    --ddp_timeout 72000 \
    --save_steps 10000 \
    --dataloader_num_workers 1 \
    --gradient_checkpointing False \
    --use_flash_attention 1 \
    --use_lisa 0 \
    --disable_group_texts 0 \
    --seed 111 \
    --galore False \
    --lora_rank 8 \
    --lora_alpha_custom 16 \
    --cola_silu True \
    --cola_init True \
    --adapter_scope all \
    --sparsity 0.001 \
    --gamma 0.7 \
    --sparse_method svd \
    --sparse_svd_rank 256 \
    --svd_inverse False \
    --finetune_method cola \
    2>&1 | tee experiment_logs/train_cola_$(date +%Y%m%d_%H%M%S).log

echo "тЬЕ COLAцибхЮЛшонч╗ГхоМцИР"
sleep 30

# 5. шонч╗ГLOROцибхЮЛ
echo ""
echo "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "="
echo "ЁЯПЛя╕П шонч╗ГLOROцибхЮЛ (5/5)"
echo "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "=" "="
python examples/finetune.py \
    --model_name_or_path "meta-llama/Llama-3.2-1B" \
    --dataset_path "data/OwLore_Dataset/merge" \
    --output_dir output_models/my_finetuned_model_loro \
    --overwrite_output_dir \
    --num_train_epochs 1.0 \
    --learning_rate 3e-4 \
    --block_size 512 \
    --per_device_train_batch_size 8 \
    --gradient_accumulation_steps 1 \
    --bf16 \
    --torch_dtype bfloat16 \
    --run_name my_finetuning_run \
    --optim adamw_hf \
    --validation_split_percentage 0 \
    --logging_steps 1 \
    --do_train \
    --ddp_timeout 72000 \
    --save_steps 10000 \
    --dataloader_num_workers 1 \
    --gradient_checkpointing False \
    --use_flash_attention 1 \
    --use_lisa 0 \
    --disable_group_texts 0 \
    --seed 111 \
    --galore False \
    --lora_rank 8 \
    --lora_alpha_custom 16 \
    --cola_silu True \
    --cola_init False \
    --adapter_scope all \
    --sparsity 0.001 \
    --gamma 0.7 \
    --sparse_method svd \
    --sparse_svd_rank 256 \
    --svd_inverse False \
    --finetune_method loro \
    2>&1 | tee experiment_logs/train_loro_$(date +%Y%m%d_%H%M%S).log

echo "тЬЕ LOROцибхЮЛшонч╗ГхоМцИР"

echo ""
echo "ЁЯОЙ цЙАцЬЙцибхЮЛшонч╗ГхоМцИР!"
echo "ч╗УцЭЯцЧ╢щЧ┤: $(date)"
echo "шонч╗ГцЧех┐Чф┐ЭхнШхЬи: experiment_logs/"