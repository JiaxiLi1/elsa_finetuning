#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
OwLore å®Œæ•´å®éªŒè„šæœ¬ - è®­ç»ƒåç«‹å³è¯„ä¼°
é€‚åˆä¸‹ç­å‰å¯åŠ¨ï¼Œè‡ªåŠ¨å®Œæˆæ‰€æœ‰è®­ç»ƒå’Œè¯„ä¼°
"""

import os
import subprocess
import time
import json
from datetime import datetime

def log_message(message, log_file=None):
    """è®°å½•æ¶ˆæ¯åˆ°æ§åˆ¶å°å’Œæ—¥å¿—æ–‡ä»¶"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    formatted_message = f"[{timestamp}] {message}"
    print(formatted_message)
    
    if log_file:
        with open(log_file, 'a') as f:
            f.write(formatted_message + "\n")
            f.flush()

def run_command_with_conda(cmd, conda_env, description, log_file=None):
    """åœ¨æŒ‡å®šcondaç¯å¢ƒä¸­è¿è¡Œå‘½ä»¤å¹¶å®æ—¶è¾“å‡º"""
    log_message(f"ğŸš€ å¼€å§‹: {description}", log_file)
    log_message(f"ç¯å¢ƒ: {conda_env}", log_file)
    log_message(f"å‘½ä»¤: {' '.join(cmd)}", log_file)
    
    start_time = time.time()
    
    try:
        # æ„å»ºå¸¦condaç¯å¢ƒçš„å‘½ä»¤
        if conda_env:
            # ä½¿ç”¨bash -cæ¥æ‰§è¡Œconda activateå’Œå®é™…å‘½ä»¤
            # å…ˆsource condaçš„åˆå§‹åŒ–è„šæœ¬ï¼Œç„¶åæ¿€æ´»ç¯å¢ƒ
            # ä½¿ç”¨shlex.quoteæ¥æ­£ç¡®å¤„ç†å‘½ä»¤ä¸­çš„å¼•å·å’Œç‰¹æ®Šå­—ç¬¦
            import shlex
            escaped_cmd = ' '.join(shlex.quote(arg) for arg in cmd)
            full_cmd = [
                "bash", "-c", 
                f"source /home/rtx3090/miniconda3/etc/profile.d/conda.sh && conda activate {conda_env} && {escaped_cmd}"
            ]
        else:
            full_cmd = cmd
        
        process = subprocess.Popen(
            full_cmd, 
            stdout=subprocess.PIPE, 
            stderr=subprocess.STDOUT,
            universal_newlines=True,
            bufsize=1
        )
        
        # å®æ—¶è¾“å‡º
        output_lines = []
        for line in process.stdout:
            line = line.strip()
            if line:  # åªè¾“å‡ºéç©ºè¡Œ
                log_message(line, log_file)
                output_lines.append(line)
        
        process.wait()
        return_code = process.returncode
        
        end_time = time.time()
        duration = end_time - start_time
        
        if return_code == 0:
            log_message(f"âœ… {description} å®Œæˆ! è€—æ—¶: {duration:.1f}ç§’", log_file)
            return True, output_lines
        else:
            log_message(f"âŒ {description} å¤±è´¥! è¿”å›ç : {return_code}", log_file)
            return False, output_lines
    except Exception as e:
        log_message(f"âŒ {description} æ‰§è¡Œå¼‚å¸¸: {str(e)}", log_file)
        return False, []

def run_command(cmd, description, log_file=None):
    """è¿è¡Œå‘½ä»¤å¹¶å®æ—¶è¾“å‡ºï¼ˆä¸æŒ‡å®šcondaç¯å¢ƒï¼‰"""
    return run_command_with_conda(cmd, None, description, log_file)

def train_and_evaluate_model(config, main_log_file):
    """è®­ç»ƒå¹¶ç«‹å³è¯„ä¼°å•ä¸ªæ¨¡å‹"""
    model_name = config['name']
    log_message(f"\n{'='*60}", main_log_file)
    log_message(f"ğŸ¯ å¼€å§‹å¤„ç† {model_name} æ¨¡å‹", main_log_file)
    log_message(f"{'='*60}", main_log_file)
    
    # 1. è®­ç»ƒæ¨¡å‹
    log_message(f"ğŸ‹ï¸ ç¬¬ä¸€æ­¥: è®­ç»ƒ {model_name} æ¨¡å‹", main_log_file)
    
    train_cmd = [
        "python", "examples/finetune.py",
        "--model_name_or_path", "meta-llama/Llama-3.2-1B",
        "--dataset_path", "data/OwLore_Dataset/merge",
        "--output_dir", config["output_dir"],
        "--overwrite_output_dir",
        "--num_train_epochs", "1.0",
        "--learning_rate", "3e-4",
        "--block_size", "512",
        "--per_device_train_batch_size", "8",
        "--gradient_accumulation_steps", "1",
        "--bf16",
        "--torch_dtype", "bfloat16",
        "--run_name", f"{model_name}_training",
        "--optim", "adamw_hf",
        "--validation_split_percentage", "0",
        "--logging_steps", "1",
        "--do_train",
        "--ddp_timeout", "72000",
        "--save_steps", "10000",  # åªåœ¨æœ€åä¿å­˜
        "--dataloader_num_workers", "1",
        "--gradient_checkpointing", "False",
        "--use_flash_attention", "1",
        "--use_lisa", "0",
        "--disable_group_texts", "0",
        "--seed", "111",
        "--galore", "False",
        "--lora_rank", str(config["rank"]),
        "--lora_alpha_custom", str(config["alpha"]),
        "--cola_silu", str(config["cola_silu"]),
        "--cola_init", str(config["cola_init"]),
        "--adapter_scope", "all",
        "--sparsity", "0.001",
        "--gamma", "0.7",
        "--sparse_method", "svd",
        "--sparse_svd_rank", "256",
        "--svd_inverse", "False",
        "--finetune_method", config["method"]
    ]
    
    train_success, _ = run_command_with_conda(train_cmd, "owlore_fine", f"è®­ç»ƒ {model_name}", main_log_file)
    
    if not train_success:
        log_message(f"âŒ {model_name} è®­ç»ƒå¤±è´¥ï¼Œè·³è¿‡è¯„ä¼°", main_log_file)
        return False, False
    
    # æŸ¥æ‰¾complete_model - æ”¯æŒä¸åŒçš„ä¿å­˜ä½ç½®
    output_dir = config["output_dir"]
    complete_model_path = None
    
    # å…ˆæ£€æŸ¥ç›´æ¥çš„complete_modelç›®å½•
    direct_complete_model = os.path.join(output_dir, "complete_model")
    if os.path.exists(direct_complete_model):
        complete_model_path = direct_complete_model
    else:
        # æŸ¥æ‰¾æœ€æ–°çš„checkpointç›®å½•ä¸­çš„complete_model
        import glob
        checkpoint_dirs = glob.glob(os.path.join(output_dir, "checkpoint-*"))
        if checkpoint_dirs:
            # æŒ‰æ•°å­—æ’åºï¼Œå–æœ€æ–°çš„
            latest_checkpoint = max(checkpoint_dirs, key=lambda x: int(x.split('-')[-1]))
            checkpoint_complete_model = os.path.join(latest_checkpoint, "complete_model")
            if os.path.exists(checkpoint_complete_model):
                complete_model_path = checkpoint_complete_model
    
    if not complete_model_path:
        log_message(f"âŒ {model_name} æ‰¾ä¸åˆ°complete_modelç›®å½•", main_log_file)
        return True, False
    
    log_message(f"âœ… {model_name} è®­ç»ƒå®Œæˆï¼Œæ‰¾åˆ°complete_model: {complete_model_path}", main_log_file)
    
    # çŸ­æš‚ä¼‘æ¯
    log_message("ğŸ˜´ è®­ç»ƒå®Œæˆï¼Œä¼‘æ¯10ç§’åå¼€å§‹è¯„ä¼°...", main_log_file)
    time.sleep(10)
    
    # 2. è¯„ä¼°æ¨¡å‹
    log_message(f"ğŸ“Š ç¬¬äºŒæ­¥: è¯„ä¼° {model_name} æ¨¡å‹", main_log_file)
    
    eval_cmd = [
        "python", "run_owlore_eval.py",
        "--model_path", complete_model_path,
        "--finetune_method", config["method"],
        "--adapter_scope", "all",
        "--rank", str(config["rank"]),
        "--alpha", str(config["alpha"]),
        "--sparsity", "0.001",
        "--gamma", "0.7",
        "--sparse_method", "svd",
        "--sparse_svd_rank", "256",
        "--cola_silu", str(config["cola_silu"]).lower(),
        "--cola_init", str(config["cola_init"]).lower(),
        "--svd_inverse", "false",
        "--tasks", "boolq,piqa,hellaswag,winogrande,arc_easy,arc_challenge,openbookqa",
        "--output_path", f"final_results/{model_name}_results",
        "--num_fewshot", "5",
        "--batch_size", "auto"
    ]
    
    eval_success, _ = run_command_with_conda(eval_cmd, "owlore_eval", f"è¯„ä¼° {model_name}", main_log_file)
    
    if eval_success:
        log_message(f"âœ… {model_name} è¯„ä¼°å®Œæˆ", main_log_file)
    else:
        log_message(f"âŒ {model_name} è¯„ä¼°å¤±è´¥", main_log_file)
    
    return train_success, eval_success

def main():
    """ä¸»å‡½æ•°"""
    start_time = datetime.now()
    print("ğŸ¯ OwLore å®Œæ•´å®éªŒå¼€å§‹!")
    print(f"ğŸ“… å¼€å§‹æ—¶é—´: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print("ğŸŒ™ é€‚åˆä¸‹ç­å‰å¯åŠ¨ï¼Œè‡ªåŠ¨å®Œæˆæ‰€æœ‰è®­ç»ƒå’Œè¯„ä¼°")
    
    # åˆ›å»ºç›®å½•
    os.makedirs("experiment_logs", exist_ok=True)
    os.makedirs("final_results", exist_ok=True)
    
    # ä¸»æ—¥å¿—æ–‡ä»¶
    main_log_file = f"experiment_logs/complete_experiment_{start_time.strftime('%Y%m%d_%H%M%S')}.log"
    
    # å®éªŒé…ç½®
    configs = [
        {
            "name": "LoRA",
            "method": "lora", 
            "output_dir": "output_models/my_finetuned_model_lora",
            "rank": 8, "alpha": 16,
            "cola_silu": True, "cola_init": False
        },
        {
            "name": "ELSA_SiLU_True",
            "method": "elsa",
            "output_dir": "output_models/my_finetuned_model_elsa_silutrue", 
            "rank": 7, "alpha": 14,
            "cola_silu": True, "cola_init": False
        },
        {
            "name": "ELSA_SiLU_False",
            "method": "elsa",
            "output_dir": "output_models/my_finetuned_model_elsa_silufalse",
            "rank": 7, "alpha": 14,
            "cola_silu": False, "cola_init": False
        },
        {
            "name": "COLA",
            "method": "cola",
            "output_dir": "output_models/my_finetuned_model_cola",
            "rank": 8, "alpha": 16,
            "cola_silu": True, "cola_init": True
        },
        {
            "name": "LORO",
            "method": "loro",
            "output_dir": "output_models/my_finetuned_model_loro",
            "rank": 8, "alpha": 16,
            "cola_silu": True, "cola_init": False
        }
    ]
    
    log_message("ğŸ¯ å®éªŒé…ç½®:", main_log_file)
    for i, config in enumerate(configs, 1):
        extra_info = ""
        if config['method'] == 'cola':
            extra_info = f", cola_init={config['cola_init']}"
        elif config['method'] == 'elsa':
            extra_info = f", cola_silu={config['cola_silu']}"
        log_message(f"  {i}. {config['name']}: {config['method']} (rank={config['rank']}, alpha={config['alpha']}{extra_info})", main_log_file)
    
    # æ‰§è¡Œå®éªŒ
    results = {}
    
    for i, config in enumerate(configs, 1):
        log_message(f"\nğŸ”„ è¿›åº¦: {i}/{len(configs)}", main_log_file)
        
        train_success, eval_success = train_and_evaluate_model(config, main_log_file)
        results[config['name']] = {
            'train_success': train_success,
            'eval_success': eval_success
        }
        
        # æ¨¡å‹é—´ä¼‘æ¯
        if i < len(configs):
            log_message("ğŸ˜´ æ¨¡å‹å®Œæˆï¼Œä¼‘æ¯30ç§’åå¤„ç†ä¸‹ä¸€ä¸ªæ¨¡å‹...", main_log_file)
            time.sleep(30)
    
    # æœ€ç»ˆæŠ¥å‘Š
    end_time = datetime.now()
    total_duration = end_time - start_time
    
    log_message(f"\n{'='*60}", main_log_file)
    log_message("ğŸ‰ æ‰€æœ‰å®éªŒå®Œæˆ!", main_log_file)
    log_message(f"ğŸ“… ç»“æŸæ—¶é—´: {end_time.strftime('%Y-%m-%d %H:%M:%S')}", main_log_file)
    log_message(f"â±ï¸ æ€»è€—æ—¶: {total_duration}", main_log_file)
    log_message(f"{'='*60}", main_log_file)
    
    log_message("\nğŸ“‹ æœ€ç»ˆç»“æœ:", main_log_file)
    train_success_count = 0
    eval_success_count = 0
    
    for name, result in results.items():
        train_status = "âœ…" if result['train_success'] else "âŒ"
        eval_status = "âœ…" if result['eval_success'] else "âŒ" if result['train_success'] else "â­ï¸"
        
        log_message(f"  {name}:", main_log_file)
        log_message(f"    è®­ç»ƒ: {train_status}", main_log_file)
        log_message(f"    è¯„ä¼°: {eval_status}", main_log_file)
        
        if result['train_success']:
            train_success_count += 1
        if result['eval_success']:
            eval_success_count += 1
    
    log_message(f"\nğŸ“Š ç»Ÿè®¡:", main_log_file)
    log_message(f"  è®­ç»ƒæˆåŠŸ: {train_success_count}/{len(configs)}", main_log_file)
    log_message(f"  è¯„ä¼°æˆåŠŸ: {eval_success_count}/{len(configs)}", main_log_file)
    
    # ä¿å­˜ç»“æœ
    final_results = {
        "start_time": start_time.isoformat(),
        "end_time": end_time.isoformat(),
        "total_duration_seconds": total_duration.total_seconds(),
        "results": results
    }
    
    with open("experiment_logs/final_summary.json", "w") as f:
        json.dump(final_results, f, indent=2, ensure_ascii=False)
    
    log_message(f"\nğŸ“ æ–‡ä»¶ä½ç½®:", main_log_file)
    log_message(f"  è¯¦ç»†æ—¥å¿—: {main_log_file}", main_log_file)
    log_message(f"  ç»“æœæ‘˜è¦: experiment_logs/final_summary.json", main_log_file)
    log_message(f"  è¯„ä¼°ç»“æœ: final_results/", main_log_file)
    
    print(f"\nğŸŒŸ å®éªŒå®Œæˆ! å¯ä»¥å®‰å¿ƒä¸‹ç­äº†~ æ€»è€—æ—¶: {total_duration}")

if __name__ == "__main__":
    main()