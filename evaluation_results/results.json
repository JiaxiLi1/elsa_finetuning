{
  "results": {},
  "error": "\u8fd4\u56de\u7801: 1",
  "stderr": "2025-05-22:18:20:06 INFO     [__main__:440] Selected Tasks: ['mmlu']\n2025-05-22:18:20:06 INFO     [evaluator:185] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n2025-05-22:18:20:06 INFO     [evaluator:223] Initializing hf model, with arguments: {'pretrained': 'output/checkpoint-1000/complete_model'}\n2025-05-22:18:20:06 INFO     [models.huggingface:137] Using device 'cuda'\nTraceback (most recent call last):\n  File \"/home/rtx3090/miniconda3/envs/owlore/lib/python3.9/site-packages/transformers/utils/hub.py\", line 403, in cached_file\n    resolved_file = hf_hub_download(\n  File \"/home/rtx3090/miniconda3/envs/owlore/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py\", line 106, in _inner_fn\n    validate_repo_id(arg_value)\n  File \"/home/rtx3090/miniconda3/envs/owlore/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py\", line 154, in validate_repo_id\n    raise HFValidationError(\nhuggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': 'output/checkpoint-1000/complete_model'. Use `repo_type` argument if needed.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/rtx3090/miniconda3/envs/owlore/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/home/rtx3090/miniconda3/envs/owlore/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/home/rtx3090/code_jiaxi/OwLore/lm-evaluation-harness/lm_eval/__main__.py\", line 530, in <module>\n    cli_evaluate()\n  File \"/home/rtx3090/code_jiaxi/OwLore/lm-evaluation-harness/lm_eval/__main__.py\", line 449, in cli_evaluate\n    results = evaluator.simple_evaluate(\n  File \"/home/rtx3090/code_jiaxi/OwLore/lm-evaluation-harness/lm_eval/utils.py\", line 439, in _wrapper\n    return fn(*args, **kwargs)\n  File \"/home/rtx3090/code_jiaxi/OwLore/lm-evaluation-harness/lm_eval/evaluator.py\", line 226, in simple_evaluate\n    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(\n  File \"/home/rtx3090/code_jiaxi/OwLore/lm-evaluation-harness/lm_eval/api/model.py\", line 151, in create_from_arg_string\n    return cls(**args, **args2)\n  File \"/home/rtx3090/code_jiaxi/OwLore/lm-evaluation-harness/lm_eval/models/huggingface.py\", line 168, in __init__\n    self._get_config(\n  File \"/home/rtx3090/code_jiaxi/OwLore/lm-evaluation-harness/lm_eval/models/huggingface.py\", line 527, in _get_config\n    self._config = transformers.AutoConfig.from_pretrained(\n  File \"/home/rtx3090/miniconda3/envs/owlore/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py\", line 1054, in from_pretrained\n    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n  File \"/home/rtx3090/miniconda3/envs/owlore/lib/python3.9/site-packages/transformers/configuration_utils.py\", line 591, in get_config_dict\n    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n  File \"/home/rtx3090/miniconda3/envs/owlore/lib/python3.9/site-packages/transformers/configuration_utils.py\", line 650, in _get_config_dict\n    resolved_config_file = cached_file(\n  File \"/home/rtx3090/miniconda3/envs/owlore/lib/python3.9/site-packages/transformers/utils/hub.py\", line 469, in cached_file\n    raise EnvironmentError(\nOSError: Incorrect path_or_model_id: 'output/checkpoint-1000/complete_model'. Please provide either the path to a local folder or the repo_id of a model on the Hub.\n",
  "stdout": ""
}